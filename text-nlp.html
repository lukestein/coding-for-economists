
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2. Natural Language Processing &#8212; Coding for Economists</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="canonical" href="https://aeturrell.github.io/coding-for-economists/text-nlp.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="1. Intro to Geo-Spatial Analysis" href="geo-intro.html" />
    <link rel="prev" title="1. Introduction to Text" href="text-intro.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />


<!-- Opengraph tags -->
<meta property="og:url"         content="https://aeturrell.github.io/coding-for-economists/text-nlp.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Natural Language Processing" />
<meta property="og:description" content="Natural Language Processing  This chapter covers text analysis, also known as natural language processing. We’ll cover tokenisation of text, removing stop words" />
<meta property="og:image"       content="https://aeturrell.github.io/coding-for-economists/_static/smith_lovelace.png" />

<meta name="twitter:card" content="summary" />


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/smith_lovelace.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Coding for Economists</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Coding
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="code-preliminaries.html">
   1. Preliminaries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="code-basics.html">
   2. Basics of Coding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="code-advanced.html">
   3. Advanced Coding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="code-where.html">
   4. Options for Writing Code
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Workflow
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="wrkflow-best-practice.html">
   1. Tips for Better Coding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wrkflow-advcd-best-practice.html">
   2. Tools for Better Coding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="wrkflow-command-line.html">
   3. The Command Line
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Data
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="data-analysis-quickstart.html">
   1. Data Analysis Quickstart
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data-intro.html">
   2. Working with Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data-exploratory-analysis.html">
   3. Exploratory Data Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data-read-and-write.html">
   4. Reading and Writing Files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data-extraction.html">
   5. Extracting Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data-sharing.html">
   6. Sharing Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data-advanced.html">
   7. Advanced Data
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Data Visualisation
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="vis-intro.html">
   1. Intro to Data Visualisation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="vis-common-plots.html">
   2. Common Plots
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Econometrics
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="econmt-probability-random.html">
   1. Probability and Random Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="econmt-statistics.html">
   2. Statistics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="econmt-regression.html">
   3. Regression
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Text Analysis
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="text-intro.html">
   1. Introduction to Text
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2. Natural Language Processing
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Geospatial
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="geo-intro.html">
   1. Intro to Geo-Spatial Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="geo-vis.html">
   2. Geo-Spatial Visualisation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Mathematics with Code
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="maths-maths-coding.html">
   1. Intro to Mathematics with Code
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Automation
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="auto-research-outputs.html">
   1. Automating Research Outputs
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Coming from ...
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="coming-from-stata.html">
   Coming from Stata
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="coming-from-matlab.html">
   Coming from Matlab
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="coming-from-r.html">
   Coming from R
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="coming-from-excel.html">
   Coming from Excel
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Elsewhere
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="zreferences.html">
   Bibliography
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/text-nlp.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/aeturrell/coding-for-economists"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/aeturrell/coding-for-economists/issues/new?title=Issue%20on%20page%20%2Ftext-nlp.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/aeturrell/coding-for-economists/main?urlpath=tree/text-nlp.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/aeturrell/coding-for-economists/blob/main/text-nlp.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   2.1. Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tokenisation">
   2.2. Tokenisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenisation-with-regular-expressions">
     2.2.1. Tokenisation with regular expressions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenisation-using-nlp-tools">
     2.2.2. Tokenisation using NLP tools
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#removing-stop-words">
   2.3. Removing Stop Words
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#counting-text">
   2.4. Counting Text
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sentence-tokenisation-and-reading-in-text-as-sentences">
   2.5. Sentence Tokenisation (and reading in text as sentences)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tf-idf">
     2.5.1. TF-IDF
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#vector-inner-product-and-cosine-similarity">
       2.5.1.1. Vector Inner Product and Cosine Similarity
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#transform-versus-fit-transform">
       2.5.1.2. Transform versus Fit Transform
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#using-a-special-vocabulary">
       2.5.1.3. Using a Special Vocabulary
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#filtering-out-frequent-and-infrequent-words">
     2.5.2. Filtering Out Frequent and Infrequent Words
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#context-of-terms">
   2.6. Context of Terms
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stemming-and-lemmatisation">
   2.7. Stemming and Lemmatisation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#part-of-speech-tagging">
   2.8. Part of Speech Tagging
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#named-entity-recognition">
   2.9. Named Entity Recognition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#readability-statistics">
   2.10. Readability Statistics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#see-also">
   2.11. See Also
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#review">
   2.12. Review
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="natural-language-processing">
<h1><span class="section-number">2. </span>Natural Language Processing<a class="headerlink" href="#natural-language-processing" title="Permalink to this headline">¶</a></h1>
<p>This chapter covers text analysis, also known as natural language processing. We’ll cover tokenisation of text, removing stop words, counting words, performing other statistics on words, and analysing the parts of speech. The focus here is on English, but many of the methods-and even the libraries-are relevant to other languages too.</p>
<div class="section" id="introduction">
<h2><span class="section-number">2.1. </span>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>When doing NLP, it’s worth thinking carefully about the unit of analysis: is it a corpus, a text, a line, a paragraph, a sentence, a word, or even a character? It could also be two of these simultaneously, and working with document x token matrices is one very common way of doing NLP. Although we’ll be mixing between a few of these in this chapter, thinking about what the block of text data you’re working with will really help you keep track of what operations are being deployed and how they might interact.</p>
<p>In case it’s also useful to know, three of the most loved NLP packages are <a class="reference external" href="https://www.nltk.org/"><strong>nltk</strong></a>, <a class="reference external" href="https://spacy.io/"><strong>spaCy</strong></a>, and <a class="reference external" href="https://radimrehurek.com/gensim/"><strong>gensim</strong></a>. As you progress through the chapter, you should also bear in mind that some of the methods we’ll see are computationally expensive and you might want to fall back on simpler approaches, such as those seen in the previous chapter, if you have large volumes of text.</p>
<p>In this chapter, we’ll use a single example and using NLP on it in a few different ways. First, though, we need to read in the text data we’ll be using, part of Adam Smith’s <em>The Wealth of Nations</em> and do some light cleaning of it.</p>
<p>Initially, we’ll read in our text so that each new line appears on a different row of a <strong>pandas</strong> dataframe. We will end up working with it both as a vector of lines and, later, as a vector of lists of words. We’ll also import the packages we’ll need; remember, if you need these on your computer you may need to run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">packagename</span></code> on your own computer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">string</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="s2">&quot;https://github.com/aeturrell/coding-for-economists/raw/main/data/smith_won.txt&quot;</span><span class="p">,</span>
    <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>An Inquiry into the Nature and Causes of the...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>POWERS OF LABOUR, AND OF THE ORDER ACCORDING T...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>DISTRIBUTED AMONG THE DIFFERENT RANKS OF THE P...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>DIVISION OF LABOUR.     CHAPTER III. THAT THE ...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>THE EXTENT OF THE MARKET.     CHAPTER IV. OF T...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We need to do a bit of light text cleaning before we get on to the more in-depth natural language processing. We’ll make use of vectorised string operations as seen in the <a class="reference internal" href="text-intro.html#text-intro"><span class="std std-ref">Introduction to Text</span></a> chapter. First, we want to put everything in lower case:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>an inquiry into the nature and causes of the...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>powers of labour, and of the order according t...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>distributed among the different ranks of the p...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>division of labour.     chapter iii. that the ...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>the extent of the market.     chapter iv. of t...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Next, we’ll remove the punctuation from the text. You may not always wish to do this but it’s a good default.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">translator</span> <span class="o">=</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="o">.</span><span class="n">maketrans</span><span class="p">({</span><span class="n">x</span><span class="p">:</span> <span class="s2">&quot;&quot;</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">})</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">translate</span><span class="p">(</span><span class="n">translator</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>an inquiry into the nature and causes of the...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>powers of labour and of the order according to...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>distributed among the different ranks of the p...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>division of labour     chapter iii that the di...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>the extent of the market     chapter iv of the...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Okay, we now have rows and rows of lower case words without punctuation.</p>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>Remove all vowels from the vector of text using <code class="docutils literal notranslate"><span class="pre">str.translate</span></code>.</p>
</div>
<p>While we’re doing some text cleaning, let’s also remove the excess whitespace found in, for example, the first entry. Leaning on the cleaning methods from the previous chapter, we’ll use regular expressions to do this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;\s+?\W+&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">regex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This searches for multiple whitespaces that preceede non-word characters and replaces them with a single whitespace.</p>
</div>
<div class="section" id="tokenisation">
<h2><span class="section-number">2.2. </span>Tokenisation<a class="headerlink" href="#tokenisation" title="Permalink to this headline">¶</a></h2>
<p>We’re going to now see an example of tokenisation: the process of taking blocks of text and breaking them down into tokens, most commonly a word but potentially all one and two word pairs. Note that you might sometimes see all two word pairs referred to as 2-grams, with an n-gram being all phrases of n words. There are many ways to tokenise text; we’ll look at two of the most common: using regular expressions and using pre-configured NLP packages.</p>
<div class="section" id="tokenisation-with-regular-expressions">
<h3><span class="section-number">2.2.1. </span>Tokenisation with regular expressions<a class="headerlink" href="#tokenisation-with-regular-expressions" title="Permalink to this headline">¶</a></h3>
<p>Because regular expressions excel at finding patterns in text, they can also be used to decide where to split text up into tokens. For a very simple example, let’s take the first line of our text example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">re</span>

<span class="n">word_pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;\w+&quot;</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">word_pattern</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">tokens</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;an&#39;,
 &#39;inquiry&#39;,
 &#39;into&#39;,
 &#39;the&#39;,
 &#39;nature&#39;,
 &#39;and&#39;,
 &#39;causes&#39;,
 &#39;of&#39;,
 &#39;the&#39;,
 &#39;wealth&#39;,
 &#39;of&#39;,
 &#39;nations&#39;,
 &#39;by&#39;,
 &#39;adam&#39;,
 &#39;smith&#39;,
 &#39;contents&#39;,
 &#39;introduction&#39;,
 &#39;and&#39;,
 &#39;plan&#39;,
 &#39;of&#39;,
 &#39;the&#39;,
 &#39;work&#39;,
 &#39;book&#39;,
 &#39;i&#39;,
 &#39;of&#39;,
 &#39;the&#39;,
 &#39;causes&#39;,
 &#39;of&#39;,
 &#39;improvement&#39;,
 &#39;in&#39;,
 &#39;the&#39;,
 &#39;productive&#39;]
</pre></div>
</div>
</div>
</div>
<p>This produced a split of a single line into one word tokens that are represented by a list of strings. We could have also asked for other variations, eg sentences, by asking to split at every “.”.</p>
</div>
<div class="section" id="tokenisation-using-nlp-tools">
<h3><span class="section-number">2.2.2. </span>Tokenisation using NLP tools<a class="headerlink" href="#tokenisation-using-nlp-tools" title="Permalink to this headline">¶</a></h3>
<p>Many of the NLP packages available in Python come with built-in tokenisation tools. We’ll use nltk for tokenisation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>

<span class="n">word_tokenize</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;an&#39;,
 &#39;inquiry&#39;,
 &#39;into&#39;,
 &#39;the&#39;,
 &#39;nature&#39;,
 &#39;and&#39;,
 &#39;causes&#39;,
 &#39;of&#39;,
 &#39;the&#39;,
 &#39;wealth&#39;,
 &#39;of&#39;,
 &#39;nations&#39;,
 &#39;by&#39;,
 &#39;adam&#39;,
 &#39;smith&#39;,
 &#39;contents&#39;,
 &#39;introduction&#39;,
 &#39;and&#39;,
 &#39;plan&#39;,
 &#39;of&#39;,
 &#39;the&#39;,
 &#39;work&#39;,
 &#39;book&#39;,
 &#39;i&#39;,
 &#39;of&#39;,
 &#39;the&#39;,
 &#39;causes&#39;,
 &#39;of&#39;,
 &#39;improvement&#39;,
 &#39;in&#39;,
 &#39;the&#39;,
 &#39;productive&#39;]
</pre></div>
</div>
</div>
</div>
<p>We have the same results as before when we used regex. Now let’s scale this tokenisation up to our whole corpus while retaining the lines of text, giving us a structure of the form (lines x tokens):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>tokens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>an inquiry into the nature and causes of the ...</td>
      <td>[an, inquiry, into, the, nature, and, causes, ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>powers of labour and of the order according to...</td>
      <td>[powers, of, labour, and, of, the, order, acco...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>distributed among the different ranks of the p...</td>
      <td>[distributed, among, the, different, ranks, of...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>division of labour chapter iii that the divisi...</td>
      <td>[division, of, labour, chapter, iii, that, the...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>the extent of the market chapter iv of the ori...</td>
      <td>[the, extent, of, the, market, chapter, iv, of...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><strong>nltk</strong> also has a <code class="docutils literal notranslate"><span class="pre">sent_tokenize</span></code> function that tokenises sentences, although as it makes use of punctuation you must take care with what pre-cleaning of text you undertake.</p>
</div>
</div>
<div class="section" id="removing-stop-words">
<h2><span class="section-number">2.3. </span>Removing Stop Words<a class="headerlink" href="#removing-stop-words" title="Permalink to this headline">¶</a></h2>
<p>Stop words are frequent but uninformative words such as ‘that’, ‘which’, ‘the’, ‘is’, ‘and’, and ‘but’. These words tend to be very common in the English language, but knowing that they appear frequently in a corpus doesn’t really tell us much. Therefore, it is quite common to strip these ‘stop’ words out of text before doing any count-based analysis (or to use methods that implicitly ignore them). Many NLP libraries come with built-in methods that remove stop words.</p>
<p>In this example of removing stop words, we’ll use the <a class="reference external" href="https://www.nltk.org/"><strong>nltk</strong></a> library. We’ll filter out any stopwords from the first entry in the tokens columns of our dataframe. Note that stop are often an add-on to a base library, and so are not always available from installing a package alone-one often needs to download the stop words relevant to whatever language you’re working with.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>

<span class="n">stopwords</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span>
    <span class="s2">&quot;english&quot;</span>
<span class="p">)</span>  <span class="c1"># Note that you may need to download these on your machine using nltk.download() within Python</span>
<span class="n">words_filtered</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;tokens&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span>
<span class="p">]</span>
<span class="n">words_filtered</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;inquiry&#39;,
 &#39;nature&#39;,
 &#39;causes&#39;,
 &#39;wealth&#39;,
 &#39;nations&#39;,
 &#39;adam&#39;,
 &#39;smith&#39;,
 &#39;contents&#39;,
 &#39;introduction&#39;,
 &#39;plan&#39;,
 &#39;work&#39;,
 &#39;book&#39;,
 &#39;causes&#39;,
 &#39;improvement&#39;,
 &#39;productive&#39;]
</pre></div>
</div>
</div>
</div>
<p>Having filtered the first entry, we can see that words such as ‘an’ and ‘into’ have disappeared but we have retained more informative words such as ‘inquiry’ and ‘nature’. Processing one entry is not enough: we need all of the lines to have stopwords removed. So we can now scale this up to the full corpus with <strong>pandas</strong>. Just as we did above, we’ll use a list comprehension to do this: but we’ll vectorise the list comprehension across the whole “tokens” series of our dataframe.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">x</span> <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>tokens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>an inquiry into the nature and causes of the ...</td>
      <td>[inquiry, nature, causes, wealth, nations, ada...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>powers of labour and of the order according to...</td>
      <td>[powers, labour, order, according, produce, na...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>distributed among the different ranks of the p...</td>
      <td>[distributed, among, different, ranks, people,...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>division of labour chapter iii that the divisi...</td>
      <td>[division, labour, chapter, iii, division, lab...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>the extent of the market chapter iv of the ori...</td>
      <td>[extent, market, chapter, iv, origin, use, mon...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now we have a much reduced set of words in our tokens, which will make the next step of analysis more meaningful.</p>
</div>
<div class="section" id="counting-text">
<h2><span class="section-number">2.4. </span>Counting Text<a class="headerlink" href="#counting-text" title="Permalink to this headline">¶</a></h2>
<p>There are several ways of performing basic counting statistics on text. We saw one in the previous chapter, <code class="docutils literal notranslate"><span class="pre">str.count()</span></code>, but that only applies to one word at a time. Often, we’re interested in the relative counts of words in a corpus. In this section, we’ll look at two powerful ways of computing this: using the <code class="docutils literal notranslate"><span class="pre">Counter</span></code> function and via term frequenc-inverse document frequency.</p>
<p>First, <code class="docutils literal notranslate"><span class="pre">Counter</span></code>, which is a built-in Python library that does pretty much what you’d expect. Here’s a simple example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="n">fruit_list</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;apple&quot;</span><span class="p">,</span>
    <span class="s2">&quot;apple&quot;</span><span class="p">,</span>
    <span class="s2">&quot;orange&quot;</span><span class="p">,</span>
    <span class="s2">&quot;satsuma&quot;</span><span class="p">,</span>
    <span class="s2">&quot;banana&quot;</span><span class="p">,</span>
    <span class="s2">&quot;orange&quot;</span><span class="p">,</span>
    <span class="s2">&quot;mango&quot;</span><span class="p">,</span>
    <span class="s2">&quot;satsuma&quot;</span><span class="p">,</span>
    <span class="s2">&quot;orange&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">freq</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">fruit_list</span><span class="p">)</span>
<span class="n">freq</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Counter({&#39;apple&#39;: 2, &#39;orange&#39;: 3, &#39;satsuma&#39;: 2, &#39;banana&#39;: 1, &#39;mango&#39;: 1})
</pre></div>
</div>
</div>
</div>
<p>Counter returns a <code class="docutils literal notranslate"><span class="pre">collections.Counter</span></code> object where the numbers of each type in a given input list are summed. The resulting dictionnary of unique counts can be extracted using <code class="docutils literal notranslate"><span class="pre">dict(freq)</span></code>, and <code class="docutils literal notranslate"><span class="pre">Counter</span></code> has some other useful functions too including <code class="docutils literal notranslate"><span class="pre">most_common()</span></code> which, given a number <code class="docutils literal notranslate"><span class="pre">n</span></code>, returns <code class="docutils literal notranslate"><span class="pre">n</span></code> tuples of the form <code class="docutils literal notranslate"><span class="pre">(thing,</span> <span class="pre">count)</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">freq</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;orange&#39;, 3), (&#39;apple&#39;, 2), (&#39;satsuma&#39;, 2), (&#39;banana&#39;, 1), (&#39;mango&#39;, 1)]
</pre></div>
</div>
</div>
</div>
<p>Say we wanted to apply this not just to every line in our corpus separately, but to our whole corpus in one go; how would we do it? <code class="docutils literal notranslate"><span class="pre">Counter</span></code> will happily accept a list but our dataframe token column is currently a vector of lists. So we must first transform the token column to a single list of all tokens and then apply <code class="docutils literal notranslate"><span class="pre">Counter</span></code>. To achieve the former and flatten a list of lists, we’ll use <code class="docutils literal notranslate"><span class="pre">itertools</span></code> chain function which makes an iterator that returns elements from the first iterable until it is exhausted, then proceeds to the next iterable, until all of the iterables in all inputs are exhausted. For example, given <code class="docutils literal notranslate"><span class="pre">[a,</span> <span class="pre">b,</span> <span class="pre">c]</span></code> and <code class="docutils literal notranslate"><span class="pre">[d,</span> <span class="pre">e,</span> <span class="pre">f]</span></code> as arguments, this function would return <code class="docutils literal notranslate"><span class="pre">[a,</span> <span class="pre">b,</span> <span class="pre">c,</span> <span class="pre">d,</span> <span class="pre">e,</span> <span class="pre">f]</span></code>. Because this function accepts an arbitrary number of iterable arguments, we use the splat operator, aka <code class="docutils literal notranslate"><span class="pre">*</span></code>, to tell it to expect lots of different arguments. The second step using <code class="docutils literal notranslate"><span class="pre">Counter</span></code> is far more straightforward!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">itertools</span>

<span class="n">merged_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()))</span>
<span class="n">freq</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">merged_list</span><span class="p">)</span>
<span class="n">freq</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;price&#39;, 775),
 (&#39;labour&#39;, 578),
 (&#39;quantity&#39;, 389),
 (&#39;greater&#39;, 386),
 (&#39;part&#39;, 376),
 (&#39;silver&#39;, 355),
 (&#39;one&#39;, 330),
 (&#39;much&#39;, 323),
 (&#39;upon&#39;, 322),
 (&#39;may&#39;, 313)]
</pre></div>
</div>
</div>
</div>
<p>Looking at the tuples representing the 10 most words in the corpus, there are some interesting patterns. “price” and “labour” are hardly surprises, while “silver” perhaps reflects the time in which the book was written a little more. “one”, “upon”, and “may” are candidates for context-specific stopwords; while our NLTK stopwords might work well for modern text, they omit words that were once more common but that are equally uninformative to the stopwords we did use. There’s no reason why these words couldn’t be added to our list of stopwords and the process re-run.</p>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>Extend the list of stopwords to include ‘may’, ‘upon’, ‘one’, and ‘much’, re-create the filtered tokens, and compute the 10 most common terms.</p>
</div>
</div>
<div class="section" id="sentence-tokenisation-and-reading-in-text-as-sentences">
<h2><span class="section-number">2.5. </span>Sentence Tokenisation (and reading in text as sentences)<a class="headerlink" href="#sentence-tokenisation-and-reading-in-text-as-sentences" title="Permalink to this headline">¶</a></h2>
<p>So far we have been working with text that is split into lines and then tokenised into words. But working with lines of text is not always the most natural unit of analysis; sometimes sentences make more sense. So let’s now work with sentences and see an example of tokenising those.</p>
<p>First, we need to read in the text as sentences. We can’t do this with pandas, because that package is limited to tabular data or very simple delimiters (like commas).</p>
<p>If we were working with a local file on our computer, we could read it in using the following code</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;smith_won.txt&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">raw_text</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</pre></div>
</div>
<p>As it is, the text file we’d like to grab is on the web so we’ll use a package that can grab files from the internet to get hold of it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;https://github.com/aeturrell/coding-for-economists/raw/main/data/smith_won.txt&quot;</span><span class="p">)</span>
<span class="n">raw_text</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span>
<span class="n">raw_text</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;  An Inquiry into the Nature and Causes of the Wealth of Nations  by Adam Smith   Contents     INTRO&#39;
</pre></div>
</div>
</div>
</div>
<p>Great, so we have our raw text. Let’s now tokenise it using <strong>nltk</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span>

<span class="n">sent_list</span> <span class="o">=</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">raw_text</span><span class="p">)</span>
<span class="n">df_sent</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">sent_list</span><span class="p">})</span>
<span class="n">df_sent</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>An Inquiry into the Nature and Causes of the...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>BOOK I.</td>
    </tr>
    <tr>
      <th>2</th>
      <td>OF THE CAUSES OF IMPROVEMENT IN THE PRODUCTIVE...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>CHAPTER I.</td>
    </tr>
    <tr>
      <th>4</th>
      <td>OF THE DIVISION OF LABOUR.</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now we just need to apply all of the cleaning procudures we did before——that is lowering the case, removing punctuation, and removing any excess whitespace.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_sent</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_sent</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
                   <span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
                   <span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">translate</span><span class="p">(</span><span class="n">translator</span><span class="p">)</span>
                   <span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;\s+?\W+&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">regex</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">df_sent</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>an inquiry into the nature and causes of the ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>book i</td>
    </tr>
    <tr>
      <th>2</th>
      <td>of the causes of improvement in the productive...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>chapter i</td>
    </tr>
    <tr>
      <th>4</th>
      <td>of the division of labour</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We’ll use this tokenised version by sentence in the next section.</p>
<div class="section" id="tf-idf">
<h3><span class="section-number">2.5.1. </span>TF-IDF<a class="headerlink" href="#tf-idf" title="Permalink to this headline">¶</a></h3>
<p>Term frequency - inverse document frequency, often referred to as <em>tf-idf</em>, is a measure of term counts (where terms could be 1-grams, 2-grams, etc.) that is weighted to try and identify the most <em>distinctively</em> frequent terms in a given corpus. It’s made up of two parts: a term-frequency (which upweights according to counts of terms) and an inverse document frequency (which downweights terms that appear frequently across the corpus). Define <span class="math notranslate nohighlight">\(t\)</span> as a term and <span class="math notranslate nohighlight">\(d\)</span> as a document. In our example thus far, <span class="math notranslate nohighlight">\(t\)</span> has represented words while our “documents” have been lines from <em>Wealth of Nations</em>. Then a simple formula for term frequency is:</p>
<div class="math notranslate nohighlight">
\[
{\displaystyle \mathrm {tf} (t,d)={\frac {f_{t,d}}{\sum _{t'\in d}{f_{t',d}}}}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(f_{t,d}\)</span> represents the frequency of term <span class="math notranslate nohighlight">\(t\)</span> in document <span class="math notranslate nohighlight">\(d\)</span>. To compute term frequencies, we will use the <span class="xref myst"><strong>sklearn</strong></span> package, which has a function called <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="n">stopwords</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The shape of the resulting tf matrix is </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()[</span><span class="mi">500</span><span class="p">:</span><span class="mi">510</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The shape of the resulting tf matrix is (7750, 5160)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;augustine&#39;,
 &#39;aulnagers&#39;,
 &#39;austria&#39;,
 &#39;authentic&#39;,
 &#39;authenticated&#39;,
 &#39;author&#39;,
 &#39;authorises&#39;,
 &#39;authority&#39;,
 &#39;authors&#39;,
 &#39;avail&#39;]
</pre></div>
</div>
</div>
</div>
<p>This created a matrix of 5,160 terms by 7,750 “documents” (actually sentences in our example) running with more or less the default settings. The only change we made to those default settings was to pass in a list of stopwords that we used earlier. The other default settings tokenise words using a regex of “(?u)\b\w\w+\b”, assume text is lowercase, only accept n-grams in the range (1, 1), and have no limit on the maximum number of features.</p>
<p>The matrix X that comes out is of an interesting type:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">type</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>scipy.sparse.csr.csr_matrix
</pre></div>
</div>
</div>
</div>
<p>ie, it’s a <em>sparse matrix</em>. Sparse matrices are more efficient for your computer when there are many missing zeros in a matrix. They do all of the usual things that matrices (arrays) do, but are just more convenient in this case. Most notably, we can perform counts with them and we can turn them into a regular matrix using <code class="docutils literal notranslate"><span class="pre">.toarray()</span></code>.</p>
<p>Let’s do some basic stats using the matrix of counts and the <strong>matplotlib</strong> visualisation library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">counts_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span><span class="o">.</span><span class="n">T</span>
<span class="n">counts_df</span> <span class="o">=</span> <span class="n">counts_df</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">counts_df</span> <span class="o">=</span> <span class="n">counts_df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">counts_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>price       775
labour      578
quantity    389
greater     386
part        376
dtype: int64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Plot settings</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span>
    <span class="s2">&quot;https://github.com/aeturrell/coding-for-economists/raw/main/plot_style.txt&quot;</span>
<span class="p">)</span>


<span class="n">num_to_plot</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">x_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_to_plot</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">x_pos</span><span class="p">,</span> <span class="n">counts_df</span><span class="p">[:</span><span class="n">num_to_plot</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">x_pos</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">counts_df</span><span class="p">[:</span><span class="n">num_to_plot</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_to_plot</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Count of terms across all &quot;documents&quot;&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="n">num_to_plot</span><span class="si">}</span><span class="s2"> top 1-grams&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/text-nlp_44_0.png" src="_images/text-nlp_44_0.png" />
</div>
</div>
<p>Let’s see what happens when we ask only for bi-grams.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Count bigrams:</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="n">stopwords</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">bigrams_df</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span>
        <span class="n">columns</span><span class="o">=</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Plot top n 2-grams</span>
<span class="n">num_to_plot</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">x_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_to_plot</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">x_pos</span><span class="p">,</span> <span class="n">bigrams_df</span><span class="p">[:</span><span class="n">num_to_plot</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">x_pos</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">bigrams_df</span><span class="p">[:</span><span class="n">num_to_plot</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_to_plot</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Count of terms across all &quot;documents&quot;&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="n">num_to_plot</span><span class="si">}</span><span class="s2"> top 2-grams&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/text-nlp_46_0.png" src="_images/text-nlp_46_0.png" />
</div>
</div>
<p>As you might expect, the highest frequency with which 2-grams occur is less than the highest frequency with which 1-grams occur.</p>
<p>Now let’s move on to the inverse document frequency. The most common definition is</p>
<div class="math notranslate nohighlight">
\[
\mathrm{idf}(t, D) =  \log \frac{N}{|\{d \in D: t \in d\}|}
\]</div>
<p>where <span class="math notranslate nohighlight">\(D\)</span> is the set of documents, <span class="math notranslate nohighlight">\(N=|D|\)</span>, and  <span class="math notranslate nohighlight">\(|\{d \in D: t \in d\}|\)</span> is the number of documents in which <span class="math notranslate nohighlight">\(t\)</span> appears. Putting both together we have</p>
<div class="math notranslate nohighlight">
\[
\mathrm{tfidf}(t, d, D) = \mathrm{tf}(t, d) \cdot \mathrm{idf}(t, D)
\]</div>
<p>Because of power-law scaling, problems with zero-count entries, and other issues, this basic formula is often modified and the <a class="reference external" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">wikipedia page</a> for tf-idf gives a good run-down of some common options.</p>
<p>To perform tfidf with code, we’ll use another <strong>sklearn</strong> function, <code class="docutils literal notranslate"><span class="pre">TfidfVectorizer</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="n">tfidf_vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="n">stopwords</span><span class="p">,</span> <span class="n">sublinear_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">tfidf_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
<span class="n">counts_tfidf</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">tfidf_vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
    <span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="p">)</span>
<span class="c1"># Plot top n 1-grams</span>
<span class="n">num_to_plot</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">x_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_to_plot</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">x_pos</span><span class="p">,</span> <span class="n">counts_tfidf</span><span class="p">[:</span><span class="n">num_to_plot</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">x_pos</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">counts_tfidf</span><span class="p">[:</span><span class="n">num_to_plot</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_to_plot</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;tf-idf weighted terms across all &quot;documents&quot;&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="n">num_to_plot</span><span class="si">}</span><span class="s2"> top 1-grams: tf-idf; X has shape </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/text-nlp_48_0.png" src="_images/text-nlp_48_0.png" />
</div>
</div>
<p>There are small differences between this ranking of terms versus the original tf 1-gram version above. In the previous one, words such as ‘one’ were slightly higher in the ranking but their common appearance in multiple documents (lines) downweights them here. In this case, we also used the sublinear option, which uses <span class="math notranslate nohighlight">\(1+\log(\mathrm{tf})\)</span> in place of <span class="math notranslate nohighlight">\(\mathrm{tf}\)</span>.</p>
<div class="section" id="vector-inner-product-and-cosine-similarity">
<h4><span class="section-number">2.5.1.1. </span>Vector Inner Product and Cosine Similarity<a class="headerlink" href="#vector-inner-product-and-cosine-similarity" title="Permalink to this headline">¶</a></h4>
<p>Because the output of tf or tf-idf is a matrix, many possibilities related to linear algebra are opened up. In particular, we can think of this the creation of a tf-idf matrix as definining a <span class="math notranslate nohighlight">\(|t| = T\)</span> dimensional vector space that is spanned by the term vectors (which act like basis vectors). Each document in the corpus then has a vector representation in terms of the basis vectors. A consequence is that there is a sensible inner vector product defined on the space. As a demonstration, let’s look for the line in the book that is closest to the title according to this vector space. The vector for the first line is just the first row in <span class="math notranslate nohighlight">\(X\)</span>. We take the argmax of the inner product with all of the <em>other</em> line vectors to find the entry in <code class="docutils literal notranslate"><span class="pre">X</span></code> that maximises the inner product.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">max_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">max_val</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>102
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Cosine similarity is </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">max_val</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sent</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_val</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sentence </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">sent</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cosine similarity is 0.42
Sentence 0:
	an inquiry into the nature and causes of the wealth of nations by adam smith contents introduction and plan of the work book i of the causes of improvement in the productive

Sentence 1:
	the society book i of the causes of improvement in the
</pre></div>
</div>
</div>
</div>
<p>We can see from this example <em>why</em> the sentence we found is the most similar in the book to the title: it contains a phrase that is very similar to part of the title. It’s worth noting here that tf-idf (and tf) do not care about <em>word order</em>, they only care about frequency, and so sometimes the most similar sentences are not what you would expect if you were judging similarity based on concepts. Another way of saying this is that the concept of ‘similarity’ as used by tf-idf is limited.</p>
</div>
<div class="section" id="transform-versus-fit-transform">
<h4><span class="section-number">2.5.1.2. </span>Transform versus Fit Transform<a class="headerlink" href="#transform-versus-fit-transform" title="Permalink to this headline">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">fit_transform</span></code> function we’ve seen is actually performing two operations here: i) create a vector space from the basis defined by terms from the text and ii) express each document (here, a sentence) as a vector in this vector space. But there’s no reason why these two operations have to be linked. In fact, by separating out these two operations, we can do nifty things like express one text in the basis vectors of another. This is more useful in practice than you might think. It allows you to ask questions like, “which of the texts in my reference corpus is most closest to these other texts?”, and more. We would ask this question by taking the inner vector product of the matrices expressing the two corpora, and find the rows of Wealth of Nations that have the greatest cosine similarity with the other texts.</p>
<p>Let’s see an example, with some test texts.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;poverty is a trap and rearing children in it is hard and perilous&quot;</span><span class="p">,</span>
                                 <span class="s2">&quot;people in different trades can meet and develop a conspiracy which ultimately hurts consumers by raising prices&quot;</span><span class="p">]})</span>
</pre></div>
</div>
</div>
</div>
<p>Now we need to i) create the vector space, ii) express WoN in the vector space, iii) express the test texts in the vector space, iv) find which rows of the WoN match best the test texts, and v) print out those rows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="n">tfidf_vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="n">stopwords</span><span class="p">,</span> <span class="n">sublinear_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># i)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tfidf_vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_sent</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
<span class="c1"># ii)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">tfidf_vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_sent</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
<span class="c1"># iii)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">tfidf_vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
<span class="c1"># iv)</span>
<span class="n">max_index_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">X</span><span class="o">*</span><span class="n">Y</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">max_index_pos</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1012, 1768]
</pre></div>
</div>
</div>
</div>
<p>Now, armed with the rows of <span class="math notranslate nohighlight">\(X\)</span>, we are ready for the final part, v)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">y_pos</span><span class="p">,</span> <span class="n">x_pos</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">max_index_pos</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Sentence number </span><span class="si">{</span><span class="n">y_pos</span><span class="si">}</span><span class="s1">:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;        test: </span><span class="si">{</span><span class="n">df_test</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">y_pos</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;         WoN: </span><span class="si">{</span><span class="n">df_sent</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">x_pos</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="si">}</span><span class="s1"> </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sentence number 0:
        test: poverty is a trap and rearing children in it is hard and perilous
         WoN: but poverty though it does not prevent the generation is extremely unfavourable to the rearing of children 

Sentence number 1:
        test: people in different trades can meet and develop a conspiracy which ultimately hurts consumers by raising prices
         WoN: people of the same trade seldom meet together even for merriment and diversion but the conversation ends in a conspiracy against the public or in some contrivance to raise prices 
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="using-a-special-vocabulary">
<h4><span class="section-number">2.5.1.3. </span>Using a Special Vocabulary<a class="headerlink" href="#using-a-special-vocabulary" title="Permalink to this headline">¶</a></h4>
<p>But why should the basis vectors come from the terms in another text? Couldn’t they come from anywhere? The answer is, of course, yes. We could choose any set of basis vectors we liked to define our vector space, and express a text in it. For this, we need a <em>special vocabulary</em>.</p>
<p>Let’s see an example of expressing the Wealth of Nations in a particularly vocab. First, we must define our vocab:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;work&quot;</span><span class="p">,</span>
         <span class="s2">&quot;wage&quot;</span><span class="p">,</span>
         <span class="s2">&quot;labour&quot;</span><span class="p">,</span>
         <span class="s2">&quot;real price&quot;</span><span class="p">,</span>
         <span class="s2">&quot;money price&quot;</span><span class="p">,</span>
         <span class="s2">&quot;productivity&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>That done, we now plug our special vocab into <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> to tell it to ignore anything that isn’t relevant (isn’t in our vocab).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">vocabulary</span><span class="o">=</span><span class="n">vocab</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">counts_df</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_sent</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span>
        <span class="n">columns</span><span class="o">=</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Plot counts from our vocab</span>
<span class="n">num_to_plot</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">x_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_to_plot</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">x_pos</span><span class="p">,</span> <span class="n">counts_df</span><span class="p">[:</span><span class="n">num_to_plot</span><span class="p">],</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">x_pos</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">counts_df</span><span class="p">[:</span><span class="n">num_to_plot</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_to_plot</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Count of terms in corpus&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Counts of vocab words in the Wealth of Nations&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/text-nlp_62_0.png" src="_images/text-nlp_62_0.png" />
</div>
</div>
<p>Note that we did not pass <code class="docutils literal notranslate"><span class="pre">stopwords</span></code> in this case; there’s no need, because passing a <code class="docutils literal notranslate"><span class="pre">vocab</span></code> effectively says to categorise any word that is <em>not</em> in the special vocabulary as a stopword. We also still passed an n-gram range to ensure our longest n-gram, with <span class="math notranslate nohighlight">\(n=2\)</span>, was counted.</p>
</div>
</div>
<div class="section" id="filtering-out-frequent-and-infrequent-words">
<h3><span class="section-number">2.5.2. </span>Filtering Out Frequent and Infrequent Words<a class="headerlink" href="#filtering-out-frequent-and-infrequent-words" title="Permalink to this headline">¶</a></h3>
<p>As well as passing stopwords in, defining vocabularies, and limiting the n-gram range, there’s another couple of ways to cut down on the number of terms that tf-idf takes account of. The first is to use the <code class="docutils literal notranslate"><span class="pre">max_features</span></code> setting to limit how many terms are tracked (this only keeps the top terms). A second is to have frequency cut-offs, both for very frequent words and for very infrequent words (be careful of this one if you’re doing any kind of out-of-sample exercise such as forecasting.) The keywords for frequency cut-offs are <code class="docutils literal notranslate"><span class="pre">max_df</span></code> and <code class="docutils literal notranslate"><span class="pre">min_df</span></code>.</p>
</div>
</div>
<div class="section" id="context-of-terms">
<h2><span class="section-number">2.6. </span>Context of Terms<a class="headerlink" href="#context-of-terms" title="Permalink to this headline">¶</a></h2>
<p>It’s all very well counting terms, but without the context of the surrounding words, it may not be all that informative. <strong>nltk</strong> has some functions that can help us. First, we have to pass our raw text into an <strong>nltk</strong> text object.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.text</span> <span class="kn">import</span> <span class="n">Text</span>

<span class="n">w_o_n</span> <span class="o">=</span> <span class="n">Text</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">raw_text</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s imagine we’re interested in the context of a particular term, say ‘price’. We can run:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_o_n</span><span class="o">.</span><span class="n">concordance</span><span class="p">(</span><span class="s2">&quot;price&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Displaying 25 of 775 matches:
. CHAPTER V. OF THE REAL AND NOMINAL PRICE OF COMMODITIES , OR OF THEIR PRICE I
L PRICE OF COMMODITIES , OR OF THEIR PRICE IN LABOUR , AND THEIR PRICE IN MONEY
OF THEIR PRICE IN LABOUR , AND THEIR PRICE IN MONEY . CHAPTER VI . OF THE COMPO
ER VI . OF THE COMPONENT PART OF THE PRICE OF COMMODITIES . CHAPTER VII . OF TH
PTER VII . OF THE NATURAL AND MARKET PRICE OF COMMODITIES . CHAPTER VIII . OF T
 in most years nearly about the same price with the corn of England , though , 
at comes to the same thing , for the price of a great quantity of theirs . He s
one to the other , except such whose price was very considerable in proportion 
value ; or wherein consists the real price of all commodities . Secondly , what
e different parts of which this real price is composed or made up . And , lastl
e or all of these different parts of price above , and sometimes sink them belo
es which sometimes hinder the market price , that is , the actual price of comm
 market price , that is , the actual price of commodities , from coinciding exa
ith what may be called their natural price . I shall endeavour to explain , as 
. CHAPTER V. OF THE REAL AND NOMINAL PRICE OF COMMODITIES , OR OF THEIR PRICE I
L PRICE OF COMMODITIES , OR OF THEIR PRICE IN LABOUR , AND THEIR PRICE IN MONEY
OF THEIR PRICE IN LABOUR , AND THEIR PRICE IN MONEY . Every man is rich or poor
 value of all commodities . The real price of every thing , what every thing re
qual quantity . Labour was the first price , the original purchase money that w
is liberty , and his happiness . The price which he pays must always be the sam
ated and compared . It is their real price ; money is their nominal price only 
 real price ; money is their nominal price only . But though equal quantities o
r quantity of goods , and to him the price of labour seems to vary like that of
be said to have a real and a nominal price . Its real price may be said to cons
 real and a nominal price . Its real price may be said to consist in the quanti
</pre></div>
</div>
</div>
</div>
<p>This gives us context for all fo the occurrences of the terms. Context is useful, but there’s more than one kind. What about <em>where</em> in a text references to different ideas or terms appear? We can do that with  <em>text dispersion plot</em>, as shown below for a selection of terms.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w_o_n</span><span class="o">.</span><span class="n">dispersion_plot</span><span class="p">([</span><span class="s2">&quot;price&quot;</span><span class="p">,</span> <span class="s2">&quot;labour&quot;</span><span class="p">,</span> <span class="s2">&quot;production&quot;</span><span class="p">,</span> <span class="s2">&quot;America&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/text-nlp_70_0.png" src="_images/text-nlp_70_0.png" />
</div>
</div>
</div>
<div class="section" id="stemming-and-lemmatisation">
<h2><span class="section-number">2.7. </span>Stemming and Lemmatisation<a class="headerlink" href="#stemming-and-lemmatisation" title="Permalink to this headline">¶</a></h2>
<p>You may have wondered, in these examples, what about words that mean the same but have different endings, for example “work”, “working”,  “worked”, and “works”? In most of the examples shown, we’ve only counted one of these words and thereby could <em>underestimate</em> their prescence. If what we really want to do is capture all discussion of a topic like ‘work’,  we should really be counting every variation on the word representing that topic.</p>
<p><em>Stemming</em> is a way to do this because it takes the stem of all of these words, in this example “work”, and then counts the stems. It’s true that this is sometimes a bit more nonsensical than using the original word (think “sci” for “science”, “scientist”, and “scientific”) but it can give a more accurate take on the occurrence of a term.</p>
<p><strong>nltk</strong> includes more than one stemmer to reduce words to their roots. Let’s see what happens when we take the tokenised words and stem them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">LancasterStemmer</span>

<span class="c1"># create an object of class LancasterStemmer</span>
<span class="n">lancaster</span> <span class="o">=</span> <span class="n">LancasterStemmer</span><span class="p">()</span>

<span class="n">cleaner_text</span> <span class="o">=</span> <span class="n">raw_text</span><span class="o">.</span><span class="n">translate</span><span class="p">(</span><span class="n">translator</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

<span class="n">stem_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">lancaster</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">term</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">cleaner_text</span><span class="p">)</span>
               <span class="k">if</span> <span class="n">term</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
<span class="n">stem_tokens</span><span class="p">[</span><span class="mi">120</span><span class="p">:</span><span class="mi">135</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;iv&#39;,
 &#39;stock&#39;,
 &#39;lent&#39;,
 &#39;interest&#39;,
 &#39;chapt&#39;,
 &#39;v&#39;,
 &#39;diff&#39;,
 &#39;employ&#39;,
 &#39;capit&#39;,
 &#39;book&#39;,
 &#39;ii&#39;,
 &#39;diff&#39;,
 &#39;progress&#39;,
 &#39;op&#39;,
 &#39;diff&#39;]
</pre></div>
</div>
</div>
</div>
<p>Now we have “pric” instead of price, and “compon” instead of “compnonent”, and so on. The stemming has taken away the ends of the words, leaving us with just their stem. Let’s see if a word count following this approach will be different.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">freq</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">stem_tokens</span><span class="p">)</span>
<span class="n">freq</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;pric&#39;, 832),
 (&#39;gre&#39;, 700),
 (&#39;labo&#39;, 686),
 (&#39;part&#39;, 511),
 (&#39;quant&#39;, 442),
 (&#39;country&#39;, 388),
 (&#39;produc&#39;, 356),
 (&#39;silv&#39;, 355),
 (&#39;diff&#39;, 348),
 (&#39;on&#39;, 348)]
</pre></div>
</div>
</div>
</div>
<p>In this case, the words that are most frequent are much the same: but you can imagine this could easily have <em>not</em> been the case and, if you’re interested in fully capturing a topic, it’s a good idea to at least check a stemmed version for comparison.</p>
<p><em>Lemmatisation</em> is slightly different; it’s a bit more intelligent than just chopping off the end of the word because it considers context and converts a word to a base form, called a lemma. Let’s perform the same exercise using lemmatisation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>

<span class="c1"># create an object of class LancasterStemmer</span>
<span class="n">wnet_lemma</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>

<span class="n">lemma_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">wnet_lemma</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">term</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">cleaner_text</span><span class="p">)</span>
               <span class="k">if</span> <span class="n">term</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
<span class="n">freq</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">lemma_tokens</span><span class="p">)</span>
<span class="n">freq</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;price&#39;, 832),
 (&#39;labour&#39;, 578),
 (&#39;part&#39;, 475),
 (&#39;quantity&#39;, 442),
 (&#39;country&#39;, 388),
 (&#39;greater&#39;, 386),
 (&#39;silver&#39;, 355),
 (&#39;one&#39;, 347),
 (&#39;time&#39;, 343),
 (&#39;much&#39;, 323)]
</pre></div>
</div>
</div>
</div>
<p>The lemmatised words we’re dealing with are more <em>understandable</em> than in the case of stemming, but note that the top ten most frequent words have changed a little too.</p>
</div>
<div class="section" id="part-of-speech-tagging">
<h2><span class="section-number">2.8. </span>Part of Speech Tagging<a class="headerlink" href="#part-of-speech-tagging" title="Permalink to this headline">¶</a></h2>
<p>Sentences are made up of verbs, nouns, adjectives, pronouns, and more of the building blocks of language. Sometimes, when you’re doing text analysis, it’s useful to understand and extract only some so-called parts of speech (or PoS). The NLP tools we’ve already seen can help us to do that. In the example below, we’ll use <code class="docutils literal notranslate"><span class="pre">pos_tag</span></code> to tag the different parts of speech in a sentence of tokenised text. The function returns tuples of ‘(word, part-of-speech)’ that we can print out.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You may need to run <code class="docutils literal notranslate"><span class="pre">nltk.download('averaged_perceptron_tagger')</span></code> to use the <code class="docutils literal notranslate"><span class="pre">pos_tag</span></code> function.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">pos_tag</span>

<span class="n">example_sent</span> <span class="o">=</span> <span class="s2">&quot;If we are going to die, let us die looking like a Peruvian folk band.&quot;</span>

<span class="n">pos_tagged_words</span> <span class="o">=</span> <span class="n">pos_tag</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">example_sent</span><span class="p">))</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">pos</span> <span class="ow">in</span> <span class="n">pos_tagged_words</span><span class="p">:</span>
    <span class="k">if</span><span class="p">(</span><span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The word &quot;</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s1">&quot; is a </span><span class="si">{</span><span class="n">pos</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The word &quot;If&quot; is a IN
The word &quot;we&quot; is a PRP
The word &quot;are&quot; is a VBP
The word &quot;going&quot; is a VBG
The word &quot;to&quot; is a TO
The word &quot;die&quot; is a VB
The word &quot;let&quot; is a VB
The word &quot;us&quot; is a PRP
The word &quot;die&quot; is a VB
The word &quot;looking&quot; is a VBG
The word &quot;like&quot; is a IN
The word &quot;a&quot; is a DT
The word &quot;Peruvian&quot; is a JJ
The word &quot;folk&quot; is a NN
The word &quot;band&quot; is a NN
</pre></div>
</div>
</div>
</div>
<p><strong>nltk</strong> uses contractions to refer to the different parts of speech: IN is a preposition, PRP a personal pronoun, VBP a verb (in non 3rd person singular present), JJ is an adjective, NN a noun, and so on.</p>
<p>When might you actually use PoS tagging? You can imagine thinking about how the use of language is different or has changed across people or institutions. You might be interested in how more active language is being employed to help readers engage more with documents and reports issued by official organisations. You might be interested in removing all words that aren’t, for example, nouns before doing some further analysis.</p>
<p>When it comes to PoS tagging, <strong>nltk</strong> is far from the only option. Another very powerful NLP library, <a class="reference external" href="https://spacy.io/"><strong>spacy</strong></a> definitely warrants a mention. Like <strong>nltk</strong>, <strong>spacy</strong> requires you to install add-ons called models to perform extra tasks. To install <strong>spacy</strong>, it’s <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">spacy</span></code> and to load the most commonly used model it’s <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">spacy</span> <span class="pre">download</span> <span class="pre">en_core_web_sm</span></code>, both to be run on the command line.</p>
<p>Let’s see the same PoS example but in <strong>spacy</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>

<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">example_sent</span><span class="p">)</span>

<span class="n">pos_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([(</span><span class="n">token</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">lemma_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">pos_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">tag_</span><span class="p">)</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">],</span>
                      <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;lemma&quot;</span><span class="p">,</span> <span class="s2">&quot;pos&quot;</span><span class="p">,</span> <span class="s2">&quot;tag&quot;</span><span class="p">])</span>
<span class="n">pos_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>lemma</th>
      <th>pos</th>
      <th>tag</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>If</td>
      <td>if</td>
      <td>SCONJ</td>
      <td>IN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>we</td>
      <td>we</td>
      <td>PRON</td>
      <td>PRP</td>
    </tr>
    <tr>
      <th>2</th>
      <td>are</td>
      <td>be</td>
      <td>AUX</td>
      <td>VBP</td>
    </tr>
    <tr>
      <th>3</th>
      <td>going</td>
      <td>go</td>
      <td>VERB</td>
      <td>VBG</td>
    </tr>
    <tr>
      <th>4</th>
      <td>to</td>
      <td>to</td>
      <td>PART</td>
      <td>TO</td>
    </tr>
    <tr>
      <th>5</th>
      <td>die</td>
      <td>die</td>
      <td>VERB</td>
      <td>VB</td>
    </tr>
    <tr>
      <th>6</th>
      <td>,</td>
      <td>,</td>
      <td>PUNCT</td>
      <td>,</td>
    </tr>
    <tr>
      <th>7</th>
      <td>let</td>
      <td>let</td>
      <td>VERB</td>
      <td>VB</td>
    </tr>
    <tr>
      <th>8</th>
      <td>us</td>
      <td>we</td>
      <td>PRON</td>
      <td>PRP</td>
    </tr>
    <tr>
      <th>9</th>
      <td>die</td>
      <td>die</td>
      <td>VERB</td>
      <td>VB</td>
    </tr>
    <tr>
      <th>10</th>
      <td>looking</td>
      <td>look</td>
      <td>VERB</td>
      <td>VBG</td>
    </tr>
    <tr>
      <th>11</th>
      <td>like</td>
      <td>like</td>
      <td>ADP</td>
      <td>IN</td>
    </tr>
    <tr>
      <th>12</th>
      <td>a</td>
      <td>a</td>
      <td>DET</td>
      <td>DT</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Peruvian</td>
      <td>peruvian</td>
      <td>ADJ</td>
      <td>JJ</td>
    </tr>
    <tr>
      <th>14</th>
      <td>folk</td>
      <td>folk</td>
      <td>NOUN</td>
      <td>NN</td>
    </tr>
    <tr>
      <th>15</th>
      <td>band</td>
      <td>band</td>
      <td>NOUN</td>
      <td>NN</td>
    </tr>
    <tr>
      <th>16</th>
      <td>.</td>
      <td>.</td>
      <td>PUNCT</td>
      <td>.</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>For those brave enough for the pun, <strong>spacy</strong> also has some nifty visualisation tools.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;When you light a candle, you also cast a shadow.&quot;</span><span class="p">)</span>

<span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;dep&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><span class="tex2jax_ignore"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="en" id="0545829a36ad4883979ef68c98a923cc-0" class="displacy" width="1800" height="399.5" direction="ltr" style="max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr">
<text class="displacy-token" fill="currentColor" text-anchor="middle" y="309.5">
    <tspan class="displacy-word" fill="currentColor" x="50">When</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="50">ADV</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="309.5">
    <tspan class="displacy-word" fill="currentColor" x="225">you</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="225">PRON</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="309.5">
    <tspan class="displacy-word" fill="currentColor" x="400">light</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="400">VERB</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="309.5">
    <tspan class="displacy-word" fill="currentColor" x="575">a</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="575">DET</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="309.5">
    <tspan class="displacy-word" fill="currentColor" x="750">candle,</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="750">NOUN</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="309.5">
    <tspan class="displacy-word" fill="currentColor" x="925">you</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="925">PRON</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="309.5">
    <tspan class="displacy-word" fill="currentColor" x="1100">also</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="1100">ADV</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="309.5">
    <tspan class="displacy-word" fill="currentColor" x="1275">cast</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="1275">VERB</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="309.5">
    <tspan class="displacy-word" fill="currentColor" x="1450">a</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="1450">DET</tspan>
</text>

<text class="displacy-token" fill="currentColor" text-anchor="middle" y="309.5">
    <tspan class="displacy-word" fill="currentColor" x="1625">shadow.</tspan>
    <tspan class="displacy-tag" dy="2em" fill="currentColor" x="1625">NOUN</tspan>
</text>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-0545829a36ad4883979ef68c98a923cc-0-0" stroke-width="2px" d="M70,264.5 C70,89.5 395.0,89.5 395.0,264.5" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-0545829a36ad4883979ef68c98a923cc-0-0" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">advmod</textPath>
    </text>
    <path class="displacy-arrowhead" d="M70,266.5 L62,254.5 78,254.5" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-0545829a36ad4883979ef68c98a923cc-0-1" stroke-width="2px" d="M245,264.5 C245,177.0 390.0,177.0 390.0,264.5" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-0545829a36ad4883979ef68c98a923cc-0-1" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">nsubj</textPath>
    </text>
    <path class="displacy-arrowhead" d="M245,266.5 L237,254.5 253,254.5" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-0545829a36ad4883979ef68c98a923cc-0-2" stroke-width="2px" d="M420,264.5 C420,2.0 1275.0,2.0 1275.0,264.5" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-0545829a36ad4883979ef68c98a923cc-0-2" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">advcl</textPath>
    </text>
    <path class="displacy-arrowhead" d="M420,266.5 L412,254.5 428,254.5" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-0545829a36ad4883979ef68c98a923cc-0-3" stroke-width="2px" d="M595,264.5 C595,177.0 740.0,177.0 740.0,264.5" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-0545829a36ad4883979ef68c98a923cc-0-3" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">det</textPath>
    </text>
    <path class="displacy-arrowhead" d="M595,266.5 L587,254.5 603,254.5" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-0545829a36ad4883979ef68c98a923cc-0-4" stroke-width="2px" d="M420,264.5 C420,89.5 745.0,89.5 745.0,264.5" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-0545829a36ad4883979ef68c98a923cc-0-4" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">dobj</textPath>
    </text>
    <path class="displacy-arrowhead" d="M745.0,266.5 L753.0,254.5 737.0,254.5" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-0545829a36ad4883979ef68c98a923cc-0-5" stroke-width="2px" d="M945,264.5 C945,89.5 1270.0,89.5 1270.0,264.5" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-0545829a36ad4883979ef68c98a923cc-0-5" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">nsubj</textPath>
    </text>
    <path class="displacy-arrowhead" d="M945,266.5 L937,254.5 953,254.5" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-0545829a36ad4883979ef68c98a923cc-0-6" stroke-width="2px" d="M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-0545829a36ad4883979ef68c98a923cc-0-6" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">advmod</textPath>
    </text>
    <path class="displacy-arrowhead" d="M1120,266.5 L1112,254.5 1128,254.5" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-0545829a36ad4883979ef68c98a923cc-0-7" stroke-width="2px" d="M1470,264.5 C1470,177.0 1615.0,177.0 1615.0,264.5" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-0545829a36ad4883979ef68c98a923cc-0-7" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">det</textPath>
    </text>
    <path class="displacy-arrowhead" d="M1470,266.5 L1462,254.5 1478,254.5" fill="currentColor"/>
</g>

<g class="displacy-arrow">
    <path class="displacy-arc" id="arrow-0545829a36ad4883979ef68c98a923cc-0-8" stroke-width="2px" d="M1295,264.5 C1295,89.5 1620.0,89.5 1620.0,264.5" fill="none" stroke="currentColor"/>
    <text dy="1.25em" style="font-size: 0.8em; letter-spacing: 1px">
        <textPath xlink:href="#arrow-0545829a36ad4883979ef68c98a923cc-0-8" class="displacy-label" startOffset="50%" side="left" fill="currentColor" text-anchor="middle">dobj</textPath>
    </text>
    <path class="displacy-arrowhead" d="M1620.0,266.5 L1628.0,254.5 1612.0,254.5" fill="currentColor"/>
</g>
</svg></span></div></div>
</div>
</div>
<div class="section" id="named-entity-recognition">
<h2><span class="section-number">2.9. </span>Named Entity Recognition<a class="headerlink" href="#named-entity-recognition" title="Permalink to this headline">¶</a></h2>
<p>This is another NLP tool that helps to pick apart the parts of language, in this case it’s a method for extracting all of the entities named in a text, whether they be people, countries, cars, whatever.</p>
<p>Let’s see an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;TAE Technologies, a California-based firm building technology to generate power from nuclear fusion, said on Thursday it had raised $280 million from new and existing investors, including Google and New Enterprise Associates.&quot;</span>

<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

<span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;ent&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><span class="tex2jax_ignore"><div class="entities" style="line-height: 2.5; direction: ltr">
<mark class="entity" style="background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    TAE Technologies
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">ORG</span>
</mark>
, a 
<mark class="entity" style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    California
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">GPE</span>
</mark>
-based firm building technology to generate power from nuclear fusion, said on 
<mark class="entity" style="background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Thursday
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">DATE</span>
</mark>
 it had raised 
<mark class="entity" style="background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    $280 million
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">MONEY</span>
</mark>
 from new and existing investors, including 
<mark class="entity" style="background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Google
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">ORG</span>
</mark>
 and 
<mark class="entity" style="background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    New Enterprise Associates
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">ORG</span>
</mark>
.</div></span></div></div>
</div>
<p>Pretty impressive stuff, but a health warning that there are plenty of texts that are not quite as clean as this one! As with the PoS tagger, you can extract the named entities in a tabular format for onward use:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([(</span><span class="n">ent</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">ent</span><span class="o">.</span><span class="n">start_char</span><span class="p">,</span> <span class="n">ent</span><span class="o">.</span><span class="n">end_char</span><span class="p">,</span> <span class="n">ent</span><span class="o">.</span><span class="n">label_</span><span class="p">)</span> <span class="k">for</span> <span class="n">ent</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">ents</span><span class="p">],</span>
             <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;start_pos&quot;</span><span class="p">,</span> <span class="s2">&quot;end_pos&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>start_pos</th>
      <th>end_pos</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>TAE Technologies</td>
      <td>0</td>
      <td>16</td>
      <td>ORG</td>
    </tr>
    <tr>
      <th>1</th>
      <td>California</td>
      <td>20</td>
      <td>30</td>
      <td>GPE</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Thursday</td>
      <td>109</td>
      <td>117</td>
      <td>DATE</td>
    </tr>
    <tr>
      <th>3</th>
      <td>$280 million</td>
      <td>132</td>
      <td>144</td>
      <td>MONEY</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Google</td>
      <td>188</td>
      <td>194</td>
      <td>ORG</td>
    </tr>
    <tr>
      <th>5</th>
      <td>New Enterprise Associates</td>
      <td>199</td>
      <td>224</td>
      <td>ORG</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The table below gives the different label meanings in Named Entity Recognition:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Label</p></th>
<th class="head"><p>Meaning</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>geo</p></td>
<td><p>Geographical entity</p></td>
</tr>
<tr class="row-odd"><td><p>org</p></td>
<td><p>Organisation</p></td>
</tr>
<tr class="row-even"><td><p>per</p></td>
<td><p>Person</p></td>
</tr>
<tr class="row-odd"><td><p>gpe</p></td>
<td><p>Geopolitical entity</p></td>
</tr>
<tr class="row-even"><td><p>date</p></td>
<td><p>Time indicator</p></td>
</tr>
<tr class="row-odd"><td><p>art</p></td>
<td><p>Artifact</p></td>
</tr>
<tr class="row-even"><td><p>eve</p></td>
<td><p>Event</p></td>
</tr>
<tr class="row-odd"><td><p>nat</p></td>
<td><p>Natural phenomenon</p></td>
</tr>
<tr class="row-even"><td><p>money</p></td>
<td><p>Reference to money amount</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="readability-statistics">
<h2><span class="section-number">2.10. </span>Readability Statistics<a class="headerlink" href="#readability-statistics" title="Permalink to this headline">¶</a></h2>
<p>Like them or loathe them, readability statistics are widely used despite what flaws individual approaches may have. Let’s take a look at at a package that can compute a wide range of them, <a class="reference external" href="https://github.com/shivam5992/textstat"><strong>textstat</strong></a>. We’ll see what it can do with English, but it supports other languages too. And we won’t use all of its measures, just a few of the most well-known.</p>
<p>As ever, you will need to run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">textstat</span></code> on the command line if you don’t already have this package installed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">textstat</span>

<span class="n">test_data</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;Playing games has always been thought to be important to &quot;</span>
    <span class="s2">&quot;the development of well-balanced and creative children; &quot;</span>
    <span class="s2">&quot;however, what part, if any, they should play in the lives &quot;</span>
    <span class="s2">&quot;of adults has never been researched that deeply. I believe &quot;</span>
    <span class="s2">&quot;that playing games is every bit as important for adults &quot;</span>
    <span class="s2">&quot;as for children. Not only is taking time out to play games &quot;</span>
    <span class="s2">&quot;with our children and other adults valuable to building &quot;</span>
    <span class="s2">&quot;interpersonal relationships but is also a wonderful way &quot;</span>
    <span class="s2">&quot;to release built up tension.&quot;</span>
<span class="p">)</span>

<span class="n">stat_func_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">textstat</span><span class="o">.</span><span class="n">flesch_reading_ease</span><span class="p">,</span>
                   <span class="n">textstat</span><span class="o">.</span><span class="n">flesch_kincaid_grade</span><span class="p">,</span>
                   <span class="n">textstat</span><span class="o">.</span><span class="n">automated_readability_index</span><span class="p">,</span>
                   <span class="n">textstat</span><span class="o">.</span><span class="n">dale_chall_readability_score</span><span class="p">,</span>
                   <span class="n">textstat</span><span class="o">.</span><span class="n">difficult_words</span><span class="p">,</span>
                  <span class="p">]</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="n">fn</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span> <span class="k">for</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">stat_func_names</span><span class="p">]],</span>
                  <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">fn</span><span class="o">.</span><span class="vm">__name__</span> <span class="k">for</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">stat_func_names</span><span class="p">],</span>
                  <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;score&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
<span class="n">df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>flesch_reading_ease</th>
      <td>52.23</td>
    </tr>
    <tr>
      <th>flesch_kincaid_grade</th>
      <td>12.80</td>
    </tr>
    <tr>
      <th>automated_readability_index</th>
      <td>15.50</td>
    </tr>
    <tr>
      <th>dale_chall_readability_score</th>
      <td>6.72</td>
    </tr>
    <tr>
      <th>difficult_words</th>
      <td>9.00</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="see-also">
<h2><span class="section-number">2.11. </span>See Also<a class="headerlink" href="#see-also" title="Permalink to this headline">¶</a></h2>
<p>We’ve only scratched the surface of NLP here; there are many other libraries and methods out there. A good easy-to-use introductory NLP package that we didn’t feature is <a class="reference external" href="https://textblob.readthedocs.io/en/dev/"><strong>textblob</strong></a>. In terms of methods, we haven’t looked at noun phrase extraction or spelling correction-but <strong>textblob</strong> offers both of these.</p>
</div>
<div class="section" id="review">
<h2><span class="section-number">2.12. </span>Review<a class="headerlink" href="#review" title="Permalink to this headline">¶</a></h2>
<p>This chapter has provided an overview of some common methods in natural language processing. If you’ve worked through this chapter, you should now be comfortable:</p>
<ul class="simple">
<li><p>✅ splitting text into lines or sentences;</p></li>
<li><p>✅ tokenising text;</p></li>
<li><p>✅ removing stopwords from text;</p></li>
<li><p>✅ computing tf-idf matrices and using the vector spaces that they create for simple similarity calculations;</p></li>
<li><p>✅ disentangling the different parts of speech, including any named entities;</p></li>
<li><p>✅ stemming and lemmatising text; and</p></li>
<li><p>✅ computing statistics on the readability of text.</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "codeforecon"
        },
        kernelOptions: {
            kernelName: "codeforecon",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'codeforecon'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="text-intro.html" title="previous page"><span class="section-number">1. </span>Introduction to Text</a>
    <a class='right-next' id="next-link" href="geo-intro.html" title="next page"><span class="section-number">1. </span>Intro to Geo-Spatial Analysis</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Arthur Turrell<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            This book is available under an MIT license.
          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-189705534-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>